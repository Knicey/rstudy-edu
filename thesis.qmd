---
title: "Thesis - Draft"
author: "Nathan Yang"
format: pdf
editor: visual
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r load-pkg-data}
#| echo: false
#| message: false
#| warning: false

library(knitr)
library(readr)
library(tidyverse)
library(broom.mixed)
library(lme4)
library(broom.mixed)

integrated_ds <- readr::read_csv('data/IntegratedDataset.csv')

```


## Project Background

The goal of this project is to model academic performance in school districts across the United States through various demographic and socioeconomic factors. The data sources include the American Community Survey (ACS), the Educational Opportunity Project, the Longitudinal School Demographic Dataset (LSDD), the Common Core of Data (CCD), and the Census. The data was joined by school district and county to create a comprehensive dataset for analysis.

I created some preliminary simple linear regression models within an interactive dashboard to explore the relationship between academic performance and various socioeconomic factors. I used the shiny [@shiny] package to create the dashboard and the ggplot2 [@ggplot2] package to create visualizations for the data. I used the rsconnect [@rsconnect] package to deploy the dashboard onto shinyapps.com where it can be publicly accessible.

Due to a lack of datasets aggregated to the school district level, this project focused on expanding my earlier work by incorporating data sources with county level statistics and an improved method for joining datasets to preserve more records and develop a more comprehensive dataset for analysis. Furthermore, I incorporated hierarchical modeling to account for the nested structure of the data and explored the impact of various predictors on academic performance.

## Literature Review

The COVID-19 pandemic has had a significant impact on education, with many students experiencing disruptions in learning due to school closures and shifts to remote learning. Several studies have shown that academic performance declined during the pandemic and students from lower income areas were disproportionately affected. For example, @irwin examined the disruption in postsecondary education plans due to the pandemic and found that lower-income families were more likely to experience disruptions in learning from canceled classes. The Educational Opportunity Project (EOP) by Stanford University created a scale for measuring academic performance across all school districts in the US and found that disadvantaged students suffered larger learning loss @fahle2023. A followup study by the EOP also found that test scores recovered from 2022 to 2023 but that nonpoor students had greater gains than poor students, further widening the achievement gap between the two groups @fahle2024. This project utilizes the dataset aggregated and curated by the EOP to model academic performance across school districts and years.

## Methodology

The first step of my project was to identify the datasets that I would be using for this project. I started with only looking at datasets aggregated by school district as that would be the most granular level relevant to my research question. I started with the American Community Survey (ACS), the Educational Opportunity Project, the Common Core of Data (CCD), and the Census. I reviewed the data dictionaries for each dataset to understand the variable encoding and the extent of the data. I also reviewed the data sources to understand the data collection process and the limitations of the data.

### Data

#### Data Resources

The core dataset is from the Educational Opportunity Project @SEDA and contains academic performance data across school districts. The academic performance varaibles represent difference in grade level relative to the 2019 national average. This dataset contains academic performance variables from 2016 to 2023 aggregated across different student subgroups and subjects within 7390 school districts.

I then identified a dataset from the Census that contains mappings from school district to county @Census2021. Counties typically contain at least one school district and often several. This dataset contains the mappings for 18998 school districts. This dataset was used to join all of the county-level datasets to the academic performance dataset.

The next core datasets were the Data Profiles (DP) from the American Community Survey (ACS). These DPs contain a selection of features from various ACS datasets that are curated to provide a consistent set of features across counties. These datasets contain demographic and socioeconomic features such as income, poverty, housing, education, and employment. These datasets are aggregated at the county level and represent the 2018 to 2022 5-Year estimated statistics for 3222 counties

I did attempt to use ACS datasets that were aggregated by school district but found that the data was very sparse and did not provide enough information for analysis.

The Common Core of Data (CCD) dataset @CCD2022 contains information on membership, salaries, and revenue from local, state, and federal sources. Additionly, this dataset contains COVID emergency relief funding from the Elementary and Secondary School Emergency Relief Fund (ESSER), which was allotted funding through the Coronavirus Aid, Relief, and Economic Security (CARES) Act for the purposes of education stabilization during the pandemic. This dataset also contains funding statistics from the Governor's Emergency Education Relief Fund (GEER), which was also allotted from the CARES act and was intended to provide emergency support to schools and higher education institutions. Both of these emergency funds also had extensions (ESSER II and GEER II respectively) that were provided through additional legislation. Both funds were also allocated to schools in alignment with Title 1 funding levels which are intended to provide additional funding to schools with a high percentage of students from low-income families. This dataset is aggregated at the school district level and contains data for 19572 school districts for the 2022 fiscal year.

The Covid School Data Hub (CSDH) @CSDH2023 dataset contains self-reported data from state education agencies on learning modality and enrollment. This dataset is aggregated at the school district level and contains data for 14967 school districts for the 2020-2021 school year.

The Social Capital Project (SCP) @SCP2019 dataset contains information on social capital indicators such as family structure, religious attendance, and social trust. These indicators are measured using data such as births per married woman, religious congregations, voting turnout, and violent crimes per population. This dataset is aggregated at the county level and contains data for 3142 counties generated in 2017.

While these datasets come from a 5-year timeframe, these are variables that are relatively stable over time and are not expected to change significantly within this timeframe. The academic performance data is the most dynamic and will be the focus of the analysis. The other datasets will be used to provide context and additional features for the analysis. Additionally, many of these datasets will have fields that are highly correlated with each other and will need to be pruned down to a more manageable set of features.

#### Data Curation

I first joined the datasets solely by school district name. This was a simple join that matched the exact names of the school districts. However, this method had issues as many school districts had different names in different datasets due to no common naming convention. This would result in many records not being matched and a loss of data.

Next, I reviewed the data sources I initially picked out and identified additional data sources that could be useful for this project. These came primarily from reading various research papers and reports that studied similar topics. I used an excel spreadsheet to track all of the data sources and variables of interest.

After identifying an exhaustive list of datasets and variables, I began the process of downloading and cleaning the data. I used the tidyverse [@tidyverse] package to clean and manipulate the data to prepare the datasets to be joined. For my joining process, I used fuzzy matching techniques to join records that had similar school district names but not exact matches. The metrics I used for fuzzy matching were string distance and Jaccard difference.

String distance is a metric that calculates the number of character changes needed to transform one string into another while Jaccard difference is a metric that compares how many 2-letter pairs are shared between two strings. I used the stringdist [@stringdist] package to calculate both of these metrics and determined thresholds from examining the distributions and matching strength for each metric. I then joined the datasets purely by matching state and calculated the metrics for every pair of school district names within a state. Once I had this dataset with all the potential matches, I developed an extensive filtering process to ensure I got the most accurate matches possible.

1.  Filter for matches that both begin with the same letter: This prevents matches names containing North/South and East/West at the beginning are not accidentally mapped together due to the characters in these cardinal directions being similar
2.  Filter for matches that end with the same three letters: This prevents matches such as "Abcdefgh county" and "Abcdefgh city" where the school districts may have the same name but are clearly different entities. This also resolves matching names that have numbers at the end such as "Abcdefgh 231" and "Abcdefgh 562" that clearly represent different school districts
3.  For each school district in the academic performance dataset, I find its best match based on string distance with ties broken by Jaccard difference (and ties at this stage decided randomly).

This is an example of a dataset joined between my academic performance data and a dataset from the CCD. Using these string comparison metrics, I was able to preserve many records that would have been unmatched if I performed a direct name join. It is especially noticeable with abbreviated words that these metrics help to identify matches like with "Heights" being reduced to "Hts." or "Community" being abbreviated to "Com" as shown below. Additional common abbreviations found in the school district names are "Saint" written as "St." and cardinal directions only represented by the first letter.

| seda_district | ccd_district | dist | jaccard |
|------------------|------------------|------------------|------------------|
| Beaverton Rural Schools | Beaverton Schools | 6 | 0.2727273 |
| North Daviess Community Schools | North Daviess Com Schools | 6 | 0.2580645 |
| Southern Wells Community Schools | Southern Wells Com Schools | 6 | 0.2580645 |
| North Lawrence Community Schools | North Lawrence Com Schools | 6 | 0.2500000 |
| South Harrison Community Schools | South Harrison Com Schools | 6 | 0.2500000 |
| Greenfield-Central Community Schools | Greenfield-Central Com Schools | 6 | 0.2285714 |
| Minnetonka Public School District | Minneapolis Public School District | 5 | 0.2857143 |
| Morris Area Public Schools | Moorhead Area Public Schools | 5 | 0.2758621 |
| West St. Paul-Mendota Hts.-Eagan | West St. Paul-Mendota Heights-Eagan | 5 | 0.2432432 |
| Minnesota Public School District | Minneapolis Public School District | 5 | 0.2424242 |
| North Branch Public Schools | North Branch Area Public Schools | 5 | 0.1724138 |
| Ridgefield Park School District | Ridgefield School District | 5 | 0.1666667 |
| Hamilton County CUSD 10 | Hamilton Co CUSD 10 | 4 | 0.2727273 |
| West Washington County CUD 10 | West Washington Co CUD 10 | 4 | 0.2142857 |
| Rising Sun-Ohio County Com | Rising Sun-Ohio Co Com | 4 | 0.1818182 |

: **Example of School District Matching**. This table shows the similarity between `seda_district` and `ccd_district` using a distance measure and Jaccard index.

By joining datasets by exact district name, I would have only had 4441 records with the CCD data. However, using the fuzzy matching techniques, I was able to match 4576 records. This is a 3% increase in the number of records that were matched.

Through district name joining I was only able to match about 250 school districts with ACS income data. However, using the fuzzy matching techniques, I was able to match 281 school districts. This is a 12% increase in the number of records that were matched.

Early on in my project, I only selected datasets that were aggregated by school district and it unfortunately did not prove fruitful as many of the ACS datasets I investigated had very limited data on school districts. This resulted in poor record retention for future dataset merging in addition to reduced modeling data as demonstrated by the ACS income dataset. I transitioned to identifying the counties for school districts in the academic performance dataset and then joining the datasets by county. This proved to be much more successful as I was able to use many ACS Data Profiles (DP) datasets which are a selection of curated features from various ACS datasets that have greater consistency in data. This allowed me to retain more records and have a more comprehensive dataset for analysis. Only two datasets had sufficient coverage at the school district level, the CCD and the CSDH datasets.

The new comprehensive dataset is much larger and contains more features than the previous dataset. However, the loss of granularity from school district to county may have an impact on the accuracy of the modeling. I keep this in mind throughout my modeling phase and weigh this in when interpreting the results.

The final step for this comprehensive dataset was to filter for records that did not have any missing data in the key variables. This was to ensure that the data was clean and ready for analysis.

Before filtering for missing data, the dataset contained 10842 records with academic performance data across 2019, 2022, and 2023. After applying all filters for missing data, the dataset contained 2208 records. This was a significant reduction in the number of records as many school districts did not have complete coverage across all datasets.

I analyzed the geographic distribution of the data before and after filtering for missing data.

```{r}
#| include: false

kable(x = integrated_ds |>
  group_by(stateabb) |>
  count() |>
  arrange(desc(n)),
  caption = "Geographic Distribution of Data by State Before Filtering"
)

kable(x = integrated_ds |>
  filter(!if_any(everything(), is.na)) |>
  group_by(stateabb) |>
  count() |>
  arrange(desc(n)),
  caption = "Geographic Distribution of Data by State After Filtering"
)
```

The geographic diversity is noticeably affected by this filtering process as many states have a significant reduction in the number of school districts. For example, California had 1165 records before filtering and 0 after filtering. This is due to the high number of missing values in the California data. This will be a limitation in the analysis as the data is not representative of all states. In particular, the ACS and SCP data were only available for counties in 21 states of which only 14 were in common for both datasets. Further filtering brought this down to the 10 states that were common across all datasets.

Due to this non-random pattern, standard imputation techniques such as mean imputation by state were deemed unsuitable. An alternative approach considered converting revenue data into categorical ranges (e.g., "Not Reported," "0-X," etc.), though this approach risked reducing the informative value of the continuous revenue variable.

### Modeling

#### Preparation

I first transformed the dataset into a long format using the pivot_longer() function. This allowed me to convert the wide-format data on yearly math scores into a format suitable for longitudinal modeling.

Next, I created new variables to facilitate trend analysis. I calculated the number of years since 2019 `yearsince2019` and its square (yearsqrdsince2019) to account for potential nonlinear trends over time as there is a general increase in test scores from 2022 to 2023. To standardize the scale of financial variables and put variables on a more similar scale, I converted all revenue and salary figures from raw values into millions of dollars. I also calculated per-student revenue and salaries by dividing total figures by student membership counts as school financial and membership variables had extremely high correlation with each other. Additionally, key socioeconomic metrics such as median income, mean income, and owner-occupied property values were scaled by dividing by 1,000. Percentages for instructional modes (in-person, hybrid, and virtual) were adjusted to range from 0 to 100 for better interpretability.

```{r}
#| include: false

integrated_longer <- integrated_ds |>
  filter(!if_any(everything(), is.na)) |> 
  pivot_longer(cols = starts_with("gys_mn"), names_to = "year", values_to = "gys_mn") |>
  mutate(year = str_remove(year, "gys_mn_")) |>
  mutate(year = str_remove(year, "_ol")) |>
  mutate(year = as.numeric(year)) 

integrated_longer <- integrated_longer |>
  mutate(
    yearsince2019 = year - 2019,
    yearsqrdsince2019 = yearsince2019^2,
    total_revenue = total_revenue / 1000000,
    total_fed_revenue = total_fed_revenue / 1000000,
    total_state_revenue = total_state_revenue / 1000000,
    revenue_per_student = total_revenue / membership,
    total_salaries = total_salaries / 1000000,
    total_instructional_salaries = total_instructional_salaries / 1000000,
    inst_salaries_per_student = total_instructional_salaries / membership,
    total_esser1 = total_esser1 / 1000000,
    total_esser2 = total_esser2 / 1000000,
    total_geer1 = total_geer1 / 1000000,
    total_geer2 = total_geer2 / 1000000,
    esser_per_student = (total_esser1 + total_esser2) / membership,
    geer_per_student = (total_geer1 + total_geer2) / membership,
    median_income = median_income / 1000,
    mean_income = mean_income / 1000,
    owner_occupied_value = owner_occupied_value / 1000,
    population = population / 1000,
    #Adjusting all percentages to be between 0 and 100
    share_inperson = share_inperson * 100,
    share_hybrid = share_hybrid * 100,
    share_virtual = share_virtual * 100,
  )

math_longer <- integrated_longer |>
  filter(subject == 'mth') |>
  filter(!is.na(total_revenue))
```

##### Curation of Predictors

When modeling, it is important to check for collinearity between predictors because highly correlated predictors can lead to unstable estimates and inflated standard errors. I used the `cor()` function to calculate the correlation matrix for each set of predictors in the dataset and identified highly correlated variables.

For the school modality variables `share_virtual`, `share_inperson`, and `share_hybrid`, I found that share_inperson and share_hybrid were highly correlated. I decided to remove both of these variables and just keep `share_virtual` in the model as that kept interpretations more straightforward as I could delineate between the effect of learning environments that were fully virtual or had some component of inperson learning.

For the revenue and salary variables, I found that the membership, total revenue, and total salary variables were very highly correlated. I decided to remove all the total revenue and total salary variables from the model except for `total_revenue`. Additionally, none of the per student variables except for `inst_salaries_per_student` were highly correlated with each other so I kept all but that one in the model.

Final List:
- `total_revenue`
- `revenue_per_student`
- `esser_per_student`
- `geer_per_student`

For the SCP variables, I know from the data documention of the SCP dataset that County_Level_index is a linear combination of the other social capital variables. As such, I removed all the other social capital variables from the model except for `County_Level_Index`.

Final List:
- `County_Level_Index`

For the ACS social characteristic variables, I found that all the variables regarding marital status were highly correlated with each other. I decided to remove all of these variables except for `married_household`. Additionally, I interestingly found that the variables regarding educational attainment were not highly correlated with each other. As such, I kept `over_25_highschool_degree` and `over_25_bachelors_degree` in the model.

Final List:
- `married_household` 
- `over_25_highschool_degree`
- `over_25_bachelors_degree`

For the ACS demographic variables, I found that `native_born`, `only_english`, and `non_english` were all highly correlated with each other. I decided to remove `only_english` and `non_english` from the model. Additionally, I found that `with_internet` and `with_computer` were highly correlated with each other. As such, I elected to keep `with_computer` in the model. The only racial variables that had high correlation were `white_percent` and `black_percent`. I keep all the racial variables in the model however as they are all important for understanding the demographic makeup of the school district.

Final List:
- `native_born`: Percentage of people born in the US
- `with_computer`: Percentage of households with a computer
- `white_percent`
- `black_percent`
- `hispanic_percent`
- `asian_percent`

For the ACS employment variables, I found that `no_workers` and `employment_past_year` were very highly correlated but no other variables were. As such I kept all the employment variables except `employment_past_year` in the model.

Final List:
- `no_workers`: Percentage of households with no workers
- `one_worker`: Percentage of households with one worker
- `unemployment`: 

For the ACS income variables, `median_income` and `mean_income` jumped out as being extremely correlated with each other. I decided to stick with `median_income` because it is a more robust measure of central tendency. These income variables were also very highly correlated with `owner_occupied_value`, `SMOC`, `rent`, and `mortgage_percentage` so those variables were removed.

Final List:
- `median_income`
- `with_health_insurance`
- `poverty`
- `occupancy`


#### Two Level Modeling

I started with fitting two unconditional models: an unconditional means model and an unconditional growth model. These models were helpful in understanding the variation in math scores across school districts and over time. Examining this variance would also determine the necessity for hierarchical modeling.

##### Unconditional Means Model

The unconditional means model evaluates the variation in math scores across school districts and over time without including any predictors. The model is specified as:

$$
Y_{ij} = \alpha_0 + \mu_i + \epsilon_{ij}
$$

where $Y_{ij}$ is the math score for school district $i$ in year $j$, $\alpha_0$ is the overall mean math score, $\mu_i$ is the random effect for school district $i$, and $\epsilon_{ij}$ is the residual error.

```{r}
#| label: unconditional-means-model

umm_model <- lmer(gys_mn~ 1 + (1|sedaadmin), 
                  data = math_longer)

tidy(umm_model)
```

The model output produced an intercept of 0.086, representing the mean math score across all districts and years (in terms of standard deviations from the 2019 national average). The variance components were:

Between-district variance = 1.23

Within-district variance = 0.374

We can calculate the intraclass correlation coefficient (ICC) as the proportion of total variance due to between-district variance:

$$
ICC = \frac{\text{Between-district variance}}{\text{Between-district variance} + \text{Within-district variance}} = \frac{1.23}{1.23 + 0.374} = 0.767
$$

This indicates that 76.7% of the total variance in math scores is due to differences between school districts. As such, this supports the need for hierarchical modeling to account for the nested structure of the data.

##### Unconditional Growth Model

The unconditional growth model (UGM) extends the unconditional means model by including a linear time component `year` to estimate how math scores change over time:

$$
Y_{ij} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \mu_i + \epsilon_{ij}
$$

where $\alpha_1$ is the fixed effect of time on math scores.

```{r}
#| label: unconditional-growth-model

ugm_model <- lmer(gys_mn~ yearsince2019 + (1|sedaadmin), 
                  data = integrated_longer)

tidy(ugm_model)
```

The model yielded -0.095 as the coefficient for year, suggesting that math scores have been declining over time. The variance components were similar to the UMM model:

The between-district variance was 1.21 and within-district variance 0.336 giving us an ICC of 0.783 further supporting the need for hierarchical modeling.

Next I created an additional model that incorporated a county-level index (County_Level_Index) as a fixed effect to capture regional differences:

Equation currently has rendering issues\~

```{r}
#| label: unconditional-growth-model-county

ugmc_model <- lmer(
  gys_mn~ year + `County_Level_Index` + (1|sedaadmin), 
  data = integrated_longer
  )

summary(ugmc_model)

anova(ugm_model, ugmc_model)
```

The model output showed that the social capital index term was positive. Furthermore, the anova test showed that the inclusion of the county-level index significantly improved the model fit. The AIC and BIC both dropped considerably along with a very small p-value, indicating a better fit with the additional fixed effect.

I then introduced my quadratic term for time to account for nonlinear trends in math scores over time:

$$
Y_{ij} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \alpha_2 \times \text{yearsince2019}^2 + \mu_i + \epsilon_{ij}
$$

```{r}
#| label: unconditional-growth-model-year-yearsqrd

ugmy2_model <- lmer(
  gys_mn~ yearsince2019 + yearsqrdsince2019 + (1|sedaadmin), 
  data = integrated_longer
  )

summary(ugmy2_model)

anova(ugm_model, ugmy2_model)
```

The model output in this case showed that the quadratic term for time was positive while the linear term was negative. This is consistent with the general trend of math scores decreasing from 2019 to 2022 and then increasing from 2022 to 2023. The anova test further showed that the quadratic term significantly improved the model fit.

I then explored the effect of learning modality on math scores by including the percentage of students in virtual learning as a fixed effect in the model:

Equation has rendering issues

```{r}
#| eval: false
#| include: false

"Y_{ij} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \alpha_2 \times \text{share_virtual} + \alpha_3 \times (\text{share_virtual} \times \text{yearsince2019}) + \mu_i + \epsilon_{ij}"

```

```{r}
#| label: time-virtual-model

ugmtv_model <- lmer(
  gys_mn~ yearsince2019 + share_virtual + share_virtual:yearsince2019 + (yearsince2019|sedaadmin), 
  data = math_longer
  )

summary(ugmtv_model)
```

The model output showed that the percentage of students in virtual learning had a negative effect on math scores. The interaction term between virtual learning and time was also negative, indicating that the negative effect of virtual learning on math scores increased over time.

Models were then evaluated by AIC and BIC using anova to determine if the additional fixed effects significantly improved the model fit. The models with the additional fixed effects had lower AIC and BIC values and the anova tests showed that certain additional fixed effects significantly improved the model fit.

#### Multilevel Hierachical Modeling

I then moved on to fitting a three-level hierarchical model to account for the nested structure of the data. I have data on the school district and county level, and I wanted to account for the variation at each level.

##### Unconditional Means Model

Revising my unconditional means model to account for county-level variation:

$$
Y_{ijk} = \alpha_0 + \mu_i + \tau_j + \epsilon_{ijk}
$$

where $Y_{ijk}$ is the math score for school district $i$ in county $j$ in year $k$, $\alpha_0$ is the overall mean math score, $\mu_i$ is the random effect for school district $i$, $\tau_j$ is the random effect for county $j$, and $\epsilon_{ijk}$ is the residual error.

```{r}
#| label: unconditional-means-model-multi

umm3_model <- lmer(gys_mn~ 1 + (1|sedaadmin) + (1|`County Names`), 
                  data = math_longer)

summary(umm3_model)
```

Results - Mean math score: 0.026 - Between-district variance: 1.0675 - Between-county variance: 0.3936

##### Unconditional Growth Model

Extending the unconditional growth model to account for county-level variation:

$$
Y_{ijk} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \mu_i + \tau_j + u_{1} \times \text{yearsince2019} + u_{2} \times \text{yearsince2019}+ \epsilon_{ijk}
$$

where $u_{1}$ and $u_{2}$ are the random slopes for year at the school district and county levels, respectively.

```{r}
#| label: unconditional-growth-model-multi


ugm3_model <- lmer(gys_mn~ yearsince2019 +  (yearsince2019|sedaadmin) + (yearsince2019|`County Names`), 
                  data = math_longer)


summary(ugm3_model)
```

##### Variable Selection

I then performed variable selection to identify the most important predictors of math scores. I created a model with all the predictors (minus some very highly correlated ones) and filtered out the least significant predictors with a t-value threshold of 2.

```{r}
#| label: all-model-coefficients

all_cols <- colnames(math_longer)

ugm3ALL_model <- lmer(
  paste(
    "gys_mn ~",
    paste(
      all_cols[!all_cols %in% c(
        "gys_mn", 
        "sedaadmin", 
        "seda_district",
        "stateabb",
        "subject",
        "County Names",
        "year",
        "share_inperson",
        "total_revenue",
        "Family_Unity"
        )], 
      collapse = " + "
      ),
    "+ (yearsince2019|sedaadmin) + (yearsince2019|`County Names`)"
  ),
  data = math_longer
)

summary(ugm3ALL_model)

###Variables with t values > 2

summary(ugm3ALL_model)$coefficients |>
  as.data.frame() |>
  filter(abs(`t value`) > 2)

```

I then categorized the predictors into different groups based what they measured and analyzed the correlation between the predictors to identify potential collinearity.

###### School Modality:

Removed share_hybrid due to high correlation with share_inperson.

###### Revenue/Funding:

Removed highly correlated salary variables and retained the variables that were aggregated by student.

###### Social Capital:

Removed component variables that contribute to County_Level_Index.

###### Social Factors:

Retained married_household and kept both educational attainment variables.

###### Demographics:

Removed only_english and non_english due to correlation; retained native_born.

Maintained both white_percent and black_percent despite correlation due to their demographic importance.

###### Employment:

Removed employment_past_year due to high correlation with no_workers.

###### Income:

Removed mean_income in favor of median_income, which better aligns with interpretability.

The final model included 21 predictors and vif was also used to check for multicollinearity. The model was then evaluated using anova to determine if the model fit was improved with the reduced set of predictors. The drop in deviance test showed that the reduced model had a lower AIC and BIC as well as a very high p value, indicating a better fit.

##### Finalizing Predictors

I then once again filtered the predictors to only include those with a t-value greater than 2. This resulted in a model with 11 predictors. The model was compared to the previous model using anova to determine if the reduced set of predictors significantly improved the model fit. The drop in deviance test showed that the reduced model had a lower AIC and BIC but a very small p value which indicates that the reduced model did not significantly improve the model fit.

Key Significant Variables:

###### Time:

yearsince2019 (Negative coefficient, consistent with pandemic impact)

yearsqrdsince2019 (Positive coefficient, indicating recovery post-pandemic)

###### School Modality:

share_virtual (Negative coefficient, aligning with reduced learning efficacy in virtual settings)

###### Revenue/Funding:

esser_per_student and geer_per_student both had negative coefficients, which is in line with title 1 schools receiving more funding.

###### County-Level Demographics:

with_computer (Negative coefficient, unexpected)

sex_ratio (Negative coefficient)

asian_percent (Positive coefficient, could be due to higher asian populations in urban areas)

unemployment (Negative coefficient, expected)

median_income (Positive coefficient, expected)

##### Random Effects

Once I had finalized the fixed effects for my model, I then explored the random effects to understand the variation at the school district and county levels.

I had recurringly used (yearsince2019\|sedaadmin) and (yearsince2019\|`County Names`) to account for the variation in math scores over time at the school district and county levels and fit models without each of these to verify their necessity and contribution to the model.

I first tried fitting a model with a random effect of share_virtual at the school district level. This model failed to converge and produced an error. I then tried fitting a model with a random effect of share_virtual at the county level. This model was able to converge. This suggests that the random effect of share_virtual is better modeled at the county level rather than the school district level.

## Results

### Integrated Dataset EDA

After I created the integrated dataset, I needed to understand the distributions and relationships between the features. I used the `ggplot2` [@ggplot2] package to create histograms, scatter plots, and correlation matrices to understand the data better. I also used the `dplyr` [@dplyr] package for data manipulation and summarization.

The first step was visualizing math scores across school districts over time. I created several histograms that showed the distribution of math scores across school districts for each year contained in the data. Additionally, I examined relationships between math scores and my predictors such as school modality, socioeconomic variables, and social capital.

#### Math Score Distribution

```{r}
#| label: data-score-distribution
#| echo: false

math_scores <- integrated_ds |>
  filter(!if_any(everything(), is.na)) |> 
  filter(subject == 'mth') |>
  mutate(
    revenue_per_student = total_revenue / membership,
    esser_per_student = (total_esser1 + total_esser2) / membership,
    geer_per_student = (total_geer1 + total_geer2) / membership
  )

math_scores_long <- math_scores |>
  pivot_longer(
    cols = c(
      gys_mn_2019_ol, 
      gys_mn_2022_ol, 
      gys_mn_2023_ol
    ), 
    names_to = "Year", 
    values_to = "Math_Score"
  )

ggplot(math_scores_long, aes(x = Math_Score, fill = Year)) +
  geom_density(alpha = 0.4, color = "black") +
  labs(title = "Density Plot of Math Scores",
       subtitle = "Distribution of Math Scores in 2019, 2022, and 2023",
       x = "Math Score (Standard Deviations from 2019 National Average)",
       y = "Density") +
  scale_fill_manual(values = c("gys_mn_2019_ol" = "skyblue", "gys_mn_2022_ol" = "lightgreen", "gys_mn_2023_ol" = "lightcoral"), 
                    name = "Year", labels = c("2019", "2022", "2023")) +
  theme_minimal()
```


The distribution of math scores is noticeably shifted to the left in 2022 and 2023 compared to 2019. This indicates that the average math scores (in terms of standard deviations from the 2019 national average) across school districts decreased in this time period. This is consistent with the findings of other research that has shown a decline in academic performance during the COVID-19 pandemic.

#### Social Capital Index

This dataset also introduced new types of data of interest such as the Social Capital Index from the Social Capital Project dataset. This index is a composite measure of social capital that includes indicators such as family structure, religious attendance, social cohesion, and institutional trust. I wanted to explore the relationship between this index and academic performance.

```{r}
#| label: data-social-capital
#| echo: false

ggplot(math_scores, aes(x = County_Level_Index)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Social Capital Index",
       subtitle = "Distribution of Social Capital Index Across School Districts",
       x = "Social Capital Index",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = `County_Level_Index`, y = gys_mn_2019_ol)) +
  geom_point() + 
  labs(title = "Social Capital Index vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Social Capital Index",
       x = "Social Capital Index",
       y = "Math Score") +
  theme_bw()

```

This scatter plot shows a weak positive relationship between the social capital index and math scores in 2019. This suggests that school districts with higher social capital tend to have higher math scores. As such, this remained a key variable of interest for further analysis.

#### Technology Use

I also wanted to explore the relationship between technology use and academic performance. I wanted to examine how access to technology such as computers and the internet might impact math scores.

```{r}
#| label: data-computer-use
#| echo: false

ggplot(math_scores, aes(x = with_computer)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Computer Access",
       subtitle = "Distribution of Computer Access Across School Districts",
       x = "Percentage of Households with Computer Access",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = with_computer, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Computer Access vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Computer Access",
       x = "Percentage of Households with Computer Access",
       y = "Math Score") +
  theme_bw()
```

There does appear to be a weak positive relationship between computer access and math scores. This suggests that school districts with higher computer access tend to have higher math scores. This relationship is consistent with the idea that access to technology can positively impact academic performance.

```{r}
#| label: data-internet-use
#| echo: false

ggplot(math_scores, aes(x = with_internet)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Internet Access",
       subtitle = "Distribution of Internet Access Across School Districts",
       x = "Percentage of Households with Internet Access",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = with_internet, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Internet Access vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Internet Access",
       x = "Percentage of Households with Internet Access",
       y = "Math Score") +
  theme_bw()
```

The scatterplot for internet usage also seems to suggest a weak positive relationship with math scores. This indicates that school districts with higher internet access tend to have higher math scores.

#### Revenue and Funding

I also wanted to explore the relationship between revenue and funding and academic performance. In particular, I was interested in the COVID-19 emergency relief funding (ESSER and GEER) that was provided to schools during the pandemic. I also wanted to examine the level of funding per student and how that might impact math scores.

```{r}
#| label: data-revenue-funding
#| include: false

kable(x = math_scores |>
  select(revenue_per_student,esser_per_student, geer_per_student) |>
  summary() 
)

ggplot(math_scores, aes(x = revenue_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Revenue per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Revenue per Student",
       x = "Revenue per Student (Dollars)",
       y = "Math Score") +
  theme_bw()

ggplot(math_scores, aes(x = esser_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "ESSER Funding per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and ESSER Funding per Student",
       x = "ESSER Funding per Student (Dollars)",
       y = "Math Score") +
  theme_bw()

ggplot(math_scores, aes(x = geer_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "GEER Funding per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and GEER Funding per Student",
       x = "GEER Funding per Student (Dollars)",
       y = "Math Score") +
  theme_bw()
```

The summary statistics for each kind of funding variable show that the the vast majority of school districts fall into a similar range except for a small number of outliers. The scatter plots for revenue per student does not indicate any clear relationship with math scores. However, the scatter plots for ESSER and GEER funding per student show a distinct negative relationship with math scores. This suggests that school districts that received more emergency relief funding were associated with lower math scores.

## References
