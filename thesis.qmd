---
title: "Thesis - Draft"
author: "Nathan Yang"
format: pdf
editor: visual
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r load-pkg-data}
#| echo: false
#| message: false
#| warning: false

library(knitr)
library(readr)
library(tidyverse)
library(broom.mixed)
library(lme4)
library(broom.mixed)

integrated_ds <- readr::read_csv('data/IntegratedDataset.csv')

```

## Project Background

The goal of this project is to model academic performance in school districts across the United States through various demographic and socioeconomic factors. The data sources include the American Community Survey (ACS), the Educational Opportunity Project, the Longitudinal School Demographic Dataset (LSDD), the Common Core of Data (CCD), and the Census. The data was joined by school district and county to create a comprehensive dataset for analysis.

I created some preliminary simple linear regression models within an interactive dashboard to explore the relationship between academic performance and various socioeconomic factors. I used the shiny [@shiny] package to create the dashboard and the ggplot2 [@ggplot2] package to create visualizations for the data. I used the rsconnect [@rsconnect] package to deploy the dashboard onto shinyapps.com where it can be publicly accessible.

Due to a lack of datasets aggregated to the school district level, this project focused on expanding my earlier work by incorporating data sources with county level statistics and an improved method for joining datasets to preserve more records and develop a more comprehensive dataset for analysis. Furthermore, I incorporated hierarchical modeling to account for the nested structure of the data and explored the impact of various predictors on academic performance.

## Literature Review

The COVID-19 pandemic has had a significant impact on education, with many students experiencing disruptions in learning due to school closures and shifts to remote learning. Several studies have shown that academic performance declined during the pandemic and students from lower income areas were disproportionately affected. For example, @irwin examined the disruption in postsecondary education plans due to the pandemic and found that lower-income families were more likely to experience disruptions in learning from canceled classes. The Educational Opportunity Project (EOP) by Stanford University created a scale for measuring academic performance across all school districts in the US and found that disadvantaged students suffered larger learning loss @fahle2023. A followup study by the EOP also found that test scores recovered from 2022 to 2023 but that nonpoor students had greater gains than poor students, further widening the achievement gap between the two groups @fahle2024. This project utilizes the dataset aggregated and curated by the EOP to model academic performance across school districts and years.

## Methodology

The first step of my project was to identify the datasets that I would be using for this project. I started with only looking at datasets aggregated by school district as that would be the most granular level relevant to my research question. I started with the American Community Survey (ACS), the Educational Opportunity Project, the Common Core of Data (CCD), and the Census. I reviewed the data dictionaries for each dataset to understand the variable encoding and the extent of the data. I also reviewed the data sources to understand the data collection process and the limitations of the data.

### Data

#### Data Resources

The core dataset is from the Educational Opportunity Project @SEDA and contains academic performance data across school districts. The academic performance varaibles represent difference in grade level relative to the 2019 national average. This dataset contains academic performance variables from 2016 to 2023 aggregated across different student subgroups and subjects within 7390 school districts.

I then identified a dataset from the Census that contains mappings from school district to county @Census2021 . Counties typically contain at least one school district and often several. This dataset contains the mappings for 18998 school districts. This dataset was used to join all of the county-level datasets to the academic performance dataset.

The next core datasets were the Data Profiles (DP) from the American Community Survey (ACS). These DPs contain a selection of features from various ACS datasets that are curated to provide a consistent set of features across counties. These datasets contain demographic and socioeconomic features such as income, poverty, housing, education, and employment. These datasets are aggregated at the county level and represent the 2018 to 2022 5-Year estimated statistics for 3222 counties

I did attempt to use ACS datasets that were aggregated by school district but found that the data was very sparse and did not provide enough information for analysis.

The Common Core of Data (CCD) dataset @CCD2022 contains information on membership, salaries, and revenue from local, state, and federal sources. Additionly, this dataset contains COVID emergency relief funding from the Elementary and Secondary School Emergency Relief Fund (ESSER), which was allotted funding through the Coronavirus Aid, Relief, and Economic Security (CARES) Act for the purposes of education stabilization during the pandemic. This dataset also contains funding statistics from the Governor's Emergency Education Relief Fund (GEER), which was also allotted from the CARES act and was intended to provide emergency support to schools and higher education institutions. Both of these emergency funds also had extensions (ESSER II and GEER II respectively) that were provided through additional legislation. Both funds were also allocated to schools in alignment with Title 1 funding levels which are intended to provide additional funding to schools with a high percentage of students from low-income families. This dataset is aggregated at the school district level and contains data for 19572 school districts for the 2022 fiscal year.

The Covid School Data Hub (CSDH) dataset @CSDH2023 contains self-reported data from state education agencies on learning modality and enrollment. This dataset is aggregated at the school district level and contains data for 14967 school districts for the 2020-2021 school year.

The Social Capital Project (SCP) dataset contains information on social capital indicators such as family structure, religious attendance, and social trust. @SCP2019 uses survey data from the ACS to construct these subindices with variables such as births per married woman, religious congregations, voting turnout, and violent crimes per population. This dataset is aggregated at the county level and contains data for 3142 counties generated in 2017.

While these datasets come from a 5-year timeframe, these are variables that are relatively stable over time and are not expected to change significantly within this timeframe. The academic performance data is the most dynamic and will be the focus of the analysis. The other datasets will be used to provide context and additional features for the analysis. Additionally, many of these datasets will have fields that are highly correlated with each other and will need to be pruned down to a more manageable set of features.

#### Data Curation

I first joined the datasets solely by school district name. This was a simple join that matched the exact names of the school districts. However, this method had issues as many school districts had different names in different datasets due to no common naming convention. This would result in many records not being matched and a loss of data.

Next, I reviewed the data sources I initially picked out and identified additional data sources that could be useful for this project. These came primarily from reading various research papers and reports that studied similar topics. I used an excel spreadsheet to track all of the data sources and variables of interest.

After identifying an exhaustive list of datasets and variables, I began the process of downloading and cleaning the data. I used the tidyverse [@tidyverse] package to clean and manipulate the data to prepare the datasets to be joined. For my joining process, I used fuzzy matching techniques to join records that had similar school district names but not exact matches. The metrics I used for fuzzy matching were string distance and Jaccard difference.

String distance is a metric that calculates the number of character changes needed to transform one string into another while Jaccard difference is a metric that compares how many 2-letter pairs are shared between two strings. I used the stringdist [@stringdist] package to calculate both of these metrics and determined thresholds from examining the distributions and matching strength for each metric. I then joined the datasets purely by matching state and calculated the metrics for every pair of school district names within a state. Once I had this dataset with all the potential matches, I developed an extensive filtering process to ensure I got the most accurate matches possible.

1.  Filter for matches that both begin with the same letter: This prevents matches names containing North/South and East/West at the beginning are not accidentally mapped together due to the characters in these cardinal directions being similar
2.  Filter for matches that end with the same three letters: This prevents matches such as "Abcdefgh county" and "Abcdefgh city" where the school districts may have the same name but are clearly different entities. This also resolves matching names that have numbers at the end such as "Abcdefgh 231" and "Abcdefgh 562" that clearly represent different school districts
3.  For each school district in the academic performance dataset, I find its best match based on string distance with ties broken by Jaccard difference (and ties at this stage decided randomly).

This is an example of a dataset joined between my academic performance data and a dataset from the CCD. Using these string comparison metrics, I was able to preserve many records that would have been unmatched if I performed a direct name join. It is especially noticeable with abbreviated words that these metrics help to identify matches like with "Heights" being reduced to "Hts." or "Community" being abbreviated to "Com" as shown below. Additional common abbreviations found in the school district names are "Saint" written as "St." and cardinal directions only represented by the first letter.

| seda_district | ccd_district | dist | jaccard |
|----|----|----|----|
| Beaverton Rural Schools | Beaverton Schools | 6 | 0.2727273 |
| North Daviess Community Schools | North Daviess Com Schools | 6 | 0.2580645 |
| Southern Wells Community Schools | Southern Wells Com Schools | 6 | 0.2580645 |
| North Lawrence Community Schools | North Lawrence Com Schools | 6 | 0.2500000 |
| South Harrison Community Schools | South Harrison Com Schools | 6 | 0.2500000 |
| Greenfield-Central Community Schools | Greenfield-Central Com Schools | 6 | 0.2285714 |
| Minnetonka Public School District | Minneapolis Public School District | 5 | 0.2857143 |
| Morris Area Public Schools | Moorhead Area Public Schools | 5 | 0.2758621 |
| West St. Paul-Mendota Hts.-Eagan | West St. Paul-Mendota Heights-Eagan | 5 | 0.2432432 |
| Minnesota Public School District | Minneapolis Public School District | 5 | 0.2424242 |
| North Branch Public Schools | North Branch Area Public Schools | 5 | 0.1724138 |
| Ridgefield Park School District | Ridgefield School District | 5 | 0.1666667 |
| Hamilton County CUSD 10 | Hamilton Co CUSD 10 | 4 | 0.2727273 |
| West Washington County CUD 10 | West Washington Co CUD 10 | 4 | 0.2142857 |
| Rising Sun-Ohio County Com | Rising Sun-Ohio Co Com | 4 | 0.1818182 |

: **Example of School District Matching**. This table shows the similarity between `seda_district` and `ccd_district` using a distance measure and Jaccard index.

By joining datasets by exact district name, I would have only had 4441 records with the CCD data. However, using the fuzzy matching techniques, I was able to match 4576 records. This is a 3% increase in the number of records that were matched.

Through district name joining I was only able to match about 250 school districts with ACS income data. However, using the fuzzy matching techniques, I was able to match 281 school districts. This is a 12% increase in the number of records that were matched.

Early on in my project, I only selected datasets that were aggregated by school district and it unfortunately did not prove fruitful as many of the ACS datasets I investigated had very limited data on school districts. This resulted in poor record retention for future dataset merging in addition to reduced modeling data as demonstrated by the ACS income dataset. I transitioned to identifying the counties for school districts in the academic performance dataset and then joining the datasets by county. This proved to be much more successful as I was able to use many ACS Data Profiles (DP) datasets which are a selection of curated features from various ACS datasets that have greater consistency in data. This allowed me to retain more records and have a more comprehensive dataset for analysis. Only two datasets had sufficient coverage at the school district level, the CCD and the CSDH datasets.

The new comprehensive dataset is much larger and contains more features than the previous dataset. However, the loss of granularity from school district to county may have an impact on the accuracy of the modeling. I keep this in mind throughout my modeling phase and weigh this in when interpreting the results.

The final step for this comprehensive dataset was to filter for records that did not have any missing data in the key variables. This was to ensure that the data was clean and ready for analysis.

Before filtering for missing data, the dataset contained 10842 records with academic performance data across 2019, 2022, and 2023. After applying all filters for missing data, the dataset contained 2208 records. This was a significant reduction in the number of records as many school districts did not have complete coverage across all datasets.

I analyzed the geographic distribution of the data before and after filtering for missing data.

```{r}
#| echo: false

kable(x = integrated_ds |>
  group_by(stateabb) |>
  count() |>
  arrange(desc(n)),
  caption = "Geographic Distribution of Data by State Before Filtering"
)

kable(x = integrated_ds |>
  filter(!if_any(everything(), is.na)) |>
  group_by(stateabb) |>
  count() |>
  arrange(desc(n)),
  caption = "Geographic Distribution of Data by State After Filtering"
)
```

The geographic diversity is noticeably affected by this filtering process as many states have a significant reduction in the number of school districts. For example, California had 1165 records before filtering and 0 after filtering. This is due to the high number of missing values in the California data. This will be a limitation in the analysis as the data is not representative of all states. In particular, the ACS and SCP data were only available for counties in 21 states of which only 14 were in common for both datasets. Further filtering brought this down to the 10 states that were common across all datasets.

Due to this non-random pattern, standard imputation techniques such as mean imputation by state were deemed unsuitable. An alternative approach considered converting revenue data into categorical ranges (e.g., "Not Reported," "0-X," etc.), though this approach risked reducing the informative value of the continuous revenue variable.

### Modeling

#### Preparation

I first transformed the dataset into a long format using the pivot_longer() function. This allowed me to convert the wide-format data on yearly math scores into a format suitable for longitudinal modeling.

Next, I created new variables to facilitate trend analysis. I calculated the number of years since 2019 `yearsince2019` and its square (yearsqrdsince2019) to account for potential nonlinear trends over time as there is a general increase in test scores from 2022 to 2023. To standardize the scale of financial variables and put variables on a more similar scale, I converted all revenue and salary figures from raw values into millions of dollars. I also calculated per-student revenue and salaries by dividing total figures by student membership counts as school financial and membership variables had extremely high correlation with each other. Additionally, key socioeconomic metrics such as median income, mean income, and owner-occupied property values were scaled by dividing by 1,000. Percentages for instructional modes (in-person, hybrid, and virtual) were adjusted to range from 0 to 100 for better interpretability.

```{r}
#| include: false

integrated_longer <- integrated_ds |>
  filter(!if_any(everything(), is.na)) |> 
  pivot_longer(cols = starts_with("gys_mn"), names_to = "year", values_to = "gys_mn") |>
  mutate(year = str_remove(year, "gys_mn_")) |>
  mutate(year = str_remove(year, "_ol")) |>
  mutate(year = as.numeric(year)) 

integrated_longer <- integrated_longer |>
  mutate(
    yearsince2019 = year - 2019,
    yearsqrdsince2019 = yearsince2019^2,
    total_revenue = total_revenue / 1000000,
    total_fed_revenue = total_fed_revenue / 1000000,
    total_state_revenue = total_state_revenue / 1000000,
    revenue_per_student = total_revenue / membership,
    total_salaries = total_salaries / 1000000,
    total_instructional_salaries = total_instructional_salaries / 1000000,
    inst_salaries_per_student = total_instructional_salaries / membership,
    total_esser1 = total_esser1 / 1000000,
    total_esser2 = total_esser2 / 1000000,
    total_geer1 = total_geer1 / 1000000,
    total_geer2 = total_geer2 / 1000000,
    esser_per_student = (total_esser1 + total_esser2) / membership,
    geer_per_student = (total_geer1 + total_geer2) / membership,
    median_income = median_income / 1000,
    mean_income = mean_income / 1000,
    owner_occupied_value = owner_occupied_value / 1000,
    population = population / 1000,
    #Adjusting all percentages to be between 0 and 100
    share_inperson = share_inperson * 100,
    share_hybrid = share_hybrid * 100,
    share_virtual = share_virtual * 100,
  )

math_longer <- integrated_longer |>
  filter(subject == 'mth') |>
  filter(!is.na(total_revenue))
```

##### Curation of Predictors

When modeling, it is important to check for collinearity between predictors because highly correlated predictors can lead to unstable estimates and inflated standard errors. I used the `cor()` function to calculate the correlation matrix for each set of predictors in the dataset and identified highly correlated variables.

For the school modality variables `share_virtual`, `share_inperson`, and `share_hybrid`, I found that share_inperson and share_hybrid were highly correlated. I decided to remove both of these variables and just keep `share_virtual` in the model as that kept interpretations more straightforward as I could delineate between the effect of learning environments that were fully virtual or had some component of inperson learning.

For the revenue and salary variables, I found that the membership, total revenue, and total salary variables were very highly correlated. I decided to remove all the total revenue and total salary variables from the model except for `total_revenue`. Additionally, none of the per student variables except for `inst_salaries_per_student` were highly correlated with each other so I kept all but that one in the model.

Final List: - `total_revenue` - `revenue_per_student` - `esser_per_student` - `geer_per_student`

For the SCP variables, I know from the data documention of the SCP dataset that County_Level_index is a linear combination of the other social capital variables. As such, I removed all the other social capital variables from the model except for `County_Level_Index`.

Final List: - `County_Level_Index`

For the ACS social characteristic variables, I found that all the variables regarding marital status were highly correlated with each other. I decided to remove all of these variables except for `married_household`. Additionally, I interestingly found that the variables regarding educational attainment were not highly correlated with each other. As such, I kept `over_25_highschool_degree` and `over_25_bachelors_degree` in the model.

Final List: - `married_household` - `over_25_highschool_degree` - `over_25_bachelors_degree`

For the ACS demographic variables, I found that `native_born`, `only_english`, and `non_english` were all highly correlated with each other. I decided to remove `only_english` and `non_english` from the model. Additionally, I found that `with_internet` and `with_computer` were highly correlated with each other. As such, I elected to keep `with_computer` in the model. The only racial variables that had high correlation were `white_percent` and `black_percent`. I keep all the racial variables in the model however as they are all important for understanding the demographic makeup of the school district.

Final List: - `native_born`: Percentage of people born in the US - `with_computer`: Percentage of households with a computer - `white_percent` - `black_percent` - `hispanic_percent` - `asian_percent`

For the ACS employment variables, I found that `no_workers` and `employment_past_year` were very highly correlated but no other variables were. As such I kept all the employment variables except `employment_past_year` in the model.

Final List: - `no_workers`: Percentage of households with no workers - `one_worker`: Percentage of households with one worker - `unemployment`:

For the ACS income variables, `median_income` and `mean_income` jumped out as being extremely correlated with each other. I decided to stick with `median_income` because it is a more robust measure of central tendency. These income variables were also very highly correlated with `owner_occupied_value`, `SMOC`, `rent`, and `mortgage_percentage` so those variables were removed.

Final List: - `median_income` - `with_health_insurance` - `poverty` - `occupancy`

Once I compiled my list of variables, I then analyzed the correlation matrix for the final set of predictors to ensure that there were no highly correlated variables that could lead to multicollinearity in the model.

Following this final check, `with_health_insurance`, `poverty`, `occupancy`, `over_25_bachelors_degree`, `unemployment`, `one_worker`, and `native_born` were removed due to high correlations with other variables.

```{r}
#| include: false
math_filtered <- math_longer |>
  select(
    gys_mn,
    yearsince2019,
    yearsqrdsince2019,
    share_virtual,
    total_revenue,
    revenue_per_student,
    esser_per_student,
    geer_per_student,
    County_Level_Index,
    married_household,
    over_25_highschool_degree,
    with_computer,
    population,
    sex_ratio,
    hispanic_percent,
    white_percent,
    black_percent,
    asian_percent,
    no_workers,
    median_income
  )
```

#### Two Level Modeling

I started with fitting two unconditional models: an unconditional means model and an unconditional growth model. These models were helpful in understanding the variation in math scores across school districts and over time. Examining this variance would also determine the necessity for hierarchical modeling.

##### Unconditional Means Model

The unconditional means model evaluates the variation in math scores across school districts and over time without including any predictors. The model is specified as:

$$
Y_{ij} =a_0 + \mu_i + \epsilon_{ij}
$$

where $Y_{ij}$ is the math score for school district $i$ in year $j$, $\alpha_0$ is the overall mean math score, $\mu_i$ is the random effect for school district $i$, and $\epsilon_{ij}$ is the residual error.

To determine the necesity for hierarchical modeling, I calculated the intraclass correlation coefficient (ICC) which is the proportion of between-district variance to total variance:

$$
ICC = \frac{\text{Between-district variance}}{\text{Between-district variance} + \text{Within-district variance}} = \frac{1.23}{1.23 + 0.374} = 0.767
$$

For this model, 76.7% of the total variance in math scores is due to differences between school districts. As such, this supports the need for hierarchical modeling due to the nested structure of the data.

##### Unconditional Growth Model

The unconditional growth model (UGM) extends the unconditional means model by including a linear time component `year` to estimate how math scores change over time:

$$
Y_{ij} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \mu_i + \epsilon_{ij}
\\
\epsilon_{ij} \sim N(0, \sigma^2)\\
\mu_i \sim N(0, \sigma^2_{\mu})
$$

where $\alpha_1$ is the fixed effect of time on math scores.

Following this, I created an additional model that incorporated (County_Level_Index) as a fixed effect to capture regional differences:

$$
Y_{ij} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \beta \times \text{County\_Level\_Index} + \mu_i + \epsilon_{ij}\\
\epsilon_{ij} \sim N(0, \sigma^2)\\
\mu_i \sim N(0, \sigma^2_{\mu})
$$

#### Three Level Modeling

I then moved on to fitting a three-level hierarchical model as the data had shown considerable variance at the school district level. Multilevel hierarchical modeling would be able to explain the variance at the school district and county level and provide more accurate estimates of the fixed effects.

##### Unconditional Means Model

Revising my unconditional means model to account for county-level variation:

$$
Y_{ijk} = \alpha_0 + \mu_i + \tau_{ij} + \epsilon_{ijk}\\
\epsilon_{ijk} \sim N(0, \sigma^2)\\
\mu_i \sim N(0, \sigma^2_{\mu})\\
\tau_{ij} \sim N(0, \sigma^2_{\tau})
$$

where $Y_{ijk}$ is the math score for school district $i$ in county $j$ in year $k$, $\alpha_0$ is the overall mean math score, $\mu_i$ is the random effect for county $i$, $\tau_{ij}$ is the random effect for school district $j$ in county $i$, and $\epsilon_{ijk}$ is the residual error.

##### Unconditional Growth Model

I then extended the unconditional growth model to account for county-level variation:

TODO: Double check this equation and put it on multiple lines

$$
Y_{ijk} = \alpha_0 + \alpha_1 \times \text{yearsince2019} + \mu_i + \tau_j + u_{1} \times \text{yearsince2019} + u_{2} \times \text{yearsince2019}+ \epsilon_{ijk}\\
\epsilon_{ijk} \sim N(0, \sigma^2)\\
\mu_i \sim N(0, \sigma^2_{\mu})\\
\tau_j \sim N(0, \sigma^2_{\tau})\\
$$

where $u_{1}$ and $u_{2}$ are the random slopes for year at the school district and county levels, respectively.

##### Variable Selection

Given my previously curated list of predictors, I created a model that included all of these variables to determine their significance and effect on math scores.

```{r}
#| label: full-model
#| include: false
full_model <- lmer(
  paste(
    "gys_mn ~",
    paste(
      colnames(math_filtered),
      collapse = " + "
      ),
    "+ (yearsince2019|sedaadmin) + (yearsince2019|`County Names`)"
  ),
  data = math_longer
)

```

After which I filtered the predictors to only include those with a t-value greater than 2. I then created a new reduced model with these 8 predictors. The model was compared to the previous model using anova to determine if the reduced set of predictors significantly improved the model fit. The drop in deviance test showed that the reduced model had a lower AIC and BIC but a very small p value which indicates that the reduced model did not significantly affect the model fit.

```{r}
#| label: reduced-model
#| echo: false
sig_fixed_effects <- summary(full_model)$coefficients |>
  as.data.frame() |>
  filter(abs(`t value`) > 2)

fixed_effect_cols = c(
  "yearsince2019",
  "yearsqrdsince2019",
  "share_virtual",
  "esser_per_student",
  "geer_per_student",
  "County_Level_Index",
  "asian_percent",
  "median_income"
)

reduced_model <- lmer(
  paste(
    "gys_mn ~",
    paste(
      fixed_effect_cols,
      collapse = " + "
    ),
    "+ (yearsince2019|sedaadmin) + (yearsince2019|`County Names`)"
  ),
  data = math_longer
)
```

##### Random Effects

Once I had finalized the fixed effects for my model, I then explored the random effects to understand the variation at the school district and county levels.

For all random effects I tested, I fit a 95% confidence interval to the model coefficients to determine the significance of the random effects. In addition to the random slopes for year, I also explored random intercepts for school district and county level variables. Variables such as `share_virtual` were tested as random slopes because it is reasonable to assume that school districts or counties may have been more or less affected by the shift to virtual learning.

## Results

### Integrated Dataset Analysis

In addition to the hierachical modeling for predicting academic performance, I needed to understand the distributions and relationships between the features. I used the `ggplot2` [@ggplot2] package to create histograms, scatter plots, and other visualizations to understand the data better. I also used the `dplyr` [@dplyr] package for data manipulation and summarization.

First, I visualized math scores across school districts over time. I created several histograms that showed the distribution of math scores across school districts for each year contained in the data. Additionally, I examined relationships between math scores and my predictors such as school modality, socioeconomic variables, and social capital.

#### Math Score Distribution

```{r}
#| label: data-score-distribution
#| echo: false

math_scores <- integrated_ds |>
  filter(!if_any(everything(), is.na)) |> 
  filter(subject == 'mth') |>
  mutate(
    revenue_per_student = total_revenue / membership,
    esser_per_student = (total_esser1 + total_esser2) / membership,
    geer_per_student = (total_geer1 + total_geer2) / membership
  )

math_scores_long <- math_scores |>
  pivot_longer(
    cols = c(
      gys_mn_2019_ol, 
      gys_mn_2022_ol, 
      gys_mn_2023_ol
    ), 
    names_to = "Year", 
    values_to = "Math_Score"
  )

ggplot(math_scores_long, aes(x = Math_Score, fill = Year)) +
  geom_density(alpha = 0.4, color = "black") +
  labs(title = "Density Plot of Math Scores",
       subtitle = "Distribution of Math Scores in 2019, 2022, and 2023",
       x = "Math Score (Standard Deviations from 2019 National Average)",
       y = "Density") +
  scale_fill_manual(values = c("gys_mn_2019_ol" = "skyblue", "gys_mn_2022_ol" = "lightgreen", "gys_mn_2023_ol" = "lightcoral"), 
                    name = "Year", labels = c("2019", "2022", "2023")) +
  theme_minimal()
```

The distribution of math scores is noticeably shifted to the left in 2022 and 2023 compared to 2019. This indicates that the average math scores (in terms of standard deviations from the 2019 national average) across school districts decreased in this time period. This is consistent with the findings of other research that has shown a decline in academic performance during the COVID-19 pandemic.

#### Social Capital Index

This dataset also introduced new types of data of interest such as the Social Capital Index from the Social Capital Project dataset. This index is a composite measure of social capital that includes indicators such as family structure, religious attendance, social cohesion, and institutional trust. I wanted to explore the relationship between this index and academic performance.

```{r}
#| label: data-social-capital
#| echo: false

ggplot(math_scores, aes(x = County_Level_Index)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Social Capital Index",
       subtitle = "Distribution of Social Capital Index Across School Districts",
       x = "Social Capital Index",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = `County_Level_Index`, y = gys_mn_2019_ol)) +
  geom_point() + 
  labs(title = "Social Capital Index vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Social Capital Index",
       x = "Social Capital Index",
       y = "Math Score") +
  theme_bw()

```

This scatter plot shows a weak positive relationship between the social capital index and math scores in 2019. This suggests that school districts with higher social capital tend to have higher math scores. As such, this remained a key variable of interest for further analysis.

#### Technology Use

I also wanted to explore the relationship between technology use and academic performance. I wanted to examine how access to technology such as computers and the internet might impact math scores.

```{r}
#| label: data-computer-use
#| echo: false

ggplot(math_scores, aes(x = with_computer)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Computer Access",
       subtitle = "Distribution of Computer Access Across School Districts",
       x = "Percentage of Households with Computer Access",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = with_computer, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Computer Access vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Computer Access",
       x = "Percentage of Households with Computer Access",
       y = "Math Score") +
  theme_bw()
```

There does appear to be a weak positive relationship between computer access and math scores. This suggests that school districts with higher computer access tend to have higher math scores. This relationship is consistent with the idea that access to technology can positively impact academic performance.

```{r}
#| label: data-internet-use
#| echo: false

ggplot(math_scores, aes(x = with_internet)) +
  geom_density(alpha = 0.4, fill = "skyblue") +
  labs(title = "Density Plot of Internet Access",
       subtitle = "Distribution of Internet Access Across School Districts",
       x = "Percentage of Households with Internet Access",
       y = "Density") +
  theme_bw()

ggplot(math_scores, aes(x = with_internet, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Internet Access vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Internet Access",
       x = "Percentage of Households with Internet Access",
       y = "Math Score") +
  theme_bw()
```

The scatterplot for internet usage also seems to suggest a weak positive relationship with math scores. This indicates that school districts with higher internet access tend to have higher math scores.

#### Revenue and Funding

I also wanted to explore the relationship between revenue and funding and academic performance. In particular, I was interested in the COVID-19 emergency relief funding (ESSER and GEER) that was provided to schools during the pandemic. I also wanted to examine the level of funding per student and how that might impact math scores.

```{r}
#| label: data-revenue-funding
#| echo: false

kable(x = math_scores |>
  select(revenue_per_student,esser_per_student, geer_per_student) |>
  summary() 
)

ggplot(math_scores, aes(x = revenue_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "Revenue per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and Revenue per Student",
       x = "Revenue per Student (Dollars)",
       y = "Math Score") +
  theme_bw()

ggplot(math_scores, aes(x = esser_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "ESSER Funding per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and ESSER Funding per Student",
       x = "ESSER Funding per Student (Dollars)",
       y = "Math Score") +
  theme_bw()

ggplot(math_scores, aes(x = geer_per_student, y = gys_mn_2019_ol)) +
  geom_point() +
  labs(title = "GEER Funding per Student vs. Math Scores in 2019",
       subtitle = "Relationship Between Math Scores and GEER Funding per Student",
       x = "GEER Funding per Student (Dollars)",
       y = "Math Score") +
  theme_bw()
```

The summary statistics for each kind of funding variable show that the the vast majority of school districts fall into a similar range except for a small number of outliers. The scatter plots for revenue per student does not indicate any clear relationship with math scores. However, the scatter plots for ESSER and GEER funding per student show a distinct negative relationship with math scores. This suggests that school districts that received more emergency relief funding were associated with lower math scores.

### Two Level Modeling

#### Unconditional Means Model

```{r}
#| label: unconditional-means-model
#| echo: false

umm_model <- lmer(gys_mn~ 1 + (1|sedaadmin), 
                  data = math_longer)

tidy(umm_model)
```

The two level unconditional means model produced an intercept of 0.086, representing the mean math score across all districts and years (in terms of standard deviations from the 2019 national average). The variance components were 1.23 and 0.374 for between-district and within-district variance, respectively. As mentioned in the previous section, the ICC was 0.767, indicating that 76.7% of the total variance in math scores was due to differences between school districts and thereby justified the need for hierarchical modeling.

#### Unconditional Growth Model

```{r}
#| label: unconditional-growth-model
#| echo: false

ugm_model <- lmer(gys_mn~ yearsince2019 + (1|sedaadmin), 
                  data = integrated_longer)

tidy(ugm_model)
```

The two level unconditional growth model yielded -0.095 as the coefficient for year, showing that math scores have generally been declining over time. The variance components were similar to the UMM model with 1.21 and 0.336 for between-district and within-district variance, respectively. The ICC was 0.783, further supporting the need for hierarchical modeling.

#### Unconditional Growth Model with County-Level Index

```{r}
#| label: unconditional-growth-model-county
#| echo: false

ugmc_model <- lmer(
  gys_mn~ year + `County_Level_Index` + (1|sedaadmin), 
  data = integrated_longer
  )

tidy(ugmc_model)
```

The model output showed that the social capital index term was positive, indicating that counties with higher levels of social cohesion and institutional trust tended to have better math scores.

```{r}
#| label: anova-test
#| echo: false

anova(ugm_model, ugmc_model)
```

Furthermore, the anova test showed that the inclusion of the county-level index significantly improved the model fit. The AIC and BIC both dropped considerably along with a very small p-value, indicating a better fit with the additional fixed effect.

### Three Level Modeling

#### Unconditional Means Model

```{r}
#| label: unconditional-means-model-multi
#| echo: false

umm3_model <- lmer(gys_mn~ 1 + (1|sedaadmin) + (1|`County Names`), 
                  data = math_longer)

tidy(umm3_model)
```

The three level unconditional means model produced an intercept of 0.0257, representing the mean math score across all districts, counties, and years. The variance components were 0.627, 1.03, and 0.374 for between-county, between-district, and within-district variance, respectively.

#### Unconditional Growth Model

```{r}
#| label: unconditional-growth-model-multi
#| echo: false


ugm3_model <- lmer(gys_mn~ yearsince2019 +  (yearsince2019|sedaadmin) + (yearsince2019|`County Names`), 
                  data = math_longer)

tidy(ugm3_model)
```

The three level unconditional growth model yielded a negative coefficient for year since 2019, confirming our previous findings that math scores have been declining over time.

#### Full Model

```{r}
#| label: full-model-output
#| echo: false

tidy(full_model)
```

The full model with all curated variables produced many coefficients that were not signficant when examining the t-value. The reduced model with only the significant predictors from the full model did not significantly improve the model fit as shown by the drop in deviance test.

```{r}
#| label: reduced-model-output
#| echo: false

tidy(reduced_model)

tidy(anova(full_model, reduced_model))
```

AIC increased slightly while BIC decreased slightly with the reduced model. The p-value was very small, indicating that the reduced model did not significantly improve the model fit.

An interesting observation of note was that sex_ratio and black_percent were on the cusp of being significant predictors. Removing one from the model actually made the other significant. This suggests that there may be some correlation between these two variables that was not clearly captured in the EDA.

## Discussion

## Conclusion

## References
